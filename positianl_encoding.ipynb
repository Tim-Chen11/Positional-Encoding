{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding is a technique used primarily in transformer models to inject information about the order of tokens into the model. Since transformers process input tokens in parallel without an inherent sense of sequence, positional encodings are added to token embeddings to provide the model with information about the position of each token within the sequence.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- **Purpose:**  \n",
    "  Positional encodings allow the model to capture the order of words or tokens, which is crucial for understanding context in tasks like translation, summarization, and language modeling.\n",
    "\n",
    "- **Implementation:**  \n",
    "  There are two main approaches:\n",
    "  - **Fixed (Sinusoidal) Encoding:**  \n",
    "    Uses sine and cosine functions of different frequencies. For example, the encoding for a position \\( pos \\) at dimension \\( i \\) is defined as:\n",
    "    \\[\n",
    "    \\text{PE}(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "    \\]\n",
    "    \\[\n",
    "    \\text{PE}(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "    \\]\n",
    "    This formulation helps the model generalize to sequence lengths not seen during training.\n",
    "  \n",
    "  - **Learned Positional Encoding:**  \n",
    "    Instead of using a fixed formula, the model learns a set of positional vectors during training. These vectors are optimized alongside the token embeddings to best capture the sequence order for the task.\n",
    "\n",
    "- **Why It Matters:**  \n",
    "  Without positional encoding, a transformer would treat the input as a bag of tokens, losing the sequential information that is key to understanding language. Adding positional encodings allows the attention mechanism to take into account the relative and absolute positions of tokens.\n",
    "\n",
    "In summary, positional encoding is essential for providing transformer models with the necessary order information to process sequential data effectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
